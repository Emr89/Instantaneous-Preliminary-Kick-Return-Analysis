---
title: "Deep Learning Test"
author: "Eric Rupinski"
date: "1/29/2022"
output: html_document
---
```{r}
library(readr)
library(keras)
library(DT)
library(tidyverse)
library(tensorflow)
library(tfruns)
```

```{r}
NT <- read.csv2("NT.csv",  sep = ";")

#Data table 
datatable(NT[sample(nrow(NT),replace = FALSE,size = 0.01 * nrow(NT)),])

#summarize NT data set 
summary(NT)

#Preprocessing
#Change to matrix
NT <- NT %>% select(-X) %>% relocate(kickReturnYardage, .before = x) %>% as.matrix()

#Get rid of Column names
dimnames(NT) = NULL
```

```{r}
#Split train and test data
set.seed(153)

indx <- sample(2,
               nrow(NT),
               replace = TRUE,
               prob = c(0.9,0.1)
               )

#creating x test and train
x_train <- NT[indx==1, 2:153]
x_test <- NT[indx==2, 2:153]

y_train <- NT[indx ==1, 1]
y_test <- NT[indx ==2, 1]

#creating y test
y_test_actual <- NT[indx ==2, 1]


```

```{r}
#Note units and input shape = # of predictor variables

#Creating model
model <- keras_model_sequential()

model %>%
  layer_dense(name = "DeepLayer1",
              units = 152 ,
              activation = "relu",
              input_shape = c(152)) %>% 
  layer_dense(name = "DeepLayer2",
              units = 152 ,
              activation = "relu") %>% 
  layer_dense(name = "OutputLayer",
              units = 50 ,
              activation = "relu")
  
```

```{r}
#Compiling Model

model %>% compile(loss = "mean_absolute_error",
                  optimizer = "adam",
                  metrics = c("mean_absolute_error"))
```

```{r}
#Fitting the model

Fit_Model <- model %>%
  fit(x_train,
      y_train,
      epoch = 152,
      batchsize = 256,
      validation_split = 0.1,
      verbose =2 )
```

```{r}
model %>% 
  evaluate(x_test,y_test)
```

```{r}
#Performance Adjustments 
#Bias =  does not separate the classes of a test set well 
#Variance = overfitting , occurs when you overfit the model to the train set, and it doesn't perform well with the test set

#correcting high bias
 #Create a bigger network , (create more layers, more nodes in layers)
#train for longer, (more epochs)
# Change to a different architecture (convolution Neural Networks for image classification)

#Correcting High Variance 
  #Capture more data 
 #Augments the data 
# Regularization, drop out, batch normalization, and other techniques
```

```{r}
#Dropout
  #removes some of the nodes, and makes them a value of , usually is done with inverted dropout 
#done to fight overfitting
```

```{r}
#Implementation of Regularization and Dropout regularization

#L2 Regularization example
l2_model <- 
  keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = num_words,
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 16, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 1, activation = "sigmoid")
l2_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = list("accuracy")
)
l2_model %>% summary()

#Dropout code example 
dropout_model <- 
  keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = num_words) %>%
  layer_dropout(0.6) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.6) %>%
  layer_dense(units = 1, activation = "sigmoid")
dropout_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = list("accuracy")
)
dropout_model %>% summary()
```

```{r}
#Steps for training data

#1: Standardize the input variables
#2: Vanishing and exploding gradients , solutions initalize weight values, setting the variance of the weight matrix equal to the recipricol of the number of input nodes which is going to be multiplied by the matrix
  #if using RELU , variance of the matrix = 2/n

#3: Mini batch gradient descent, for bigger data sets values used, = powers of 2 = 16, 32, 64, 128, 256, 512
#4: Gradient Descent with momentum = using exponential weighted mean average to use the averages to determine the the position, but uses the closer data points with a higher weights, so proximity matters ,or almost Root mean square propagation, combining root square mean and momentum leaves us with ADAM (used before)

#5: learning rate decay = dont overshoot minimum 
#6 = Batch normalization normalize the weights for each hidden layers BEFORE the activation kicks in 
```

```{r}
#Implementing Training improvements:

feature.means = vector(length = ncol(x_train))

#mean
for (i in 1:length(feature.means)) {
feature.means[i] = mean(x_train[,i])
}

#sd
feature.sds = vector(length = ncol(x_train))
for (i in 1:length(feature.sds)) {
feature.sds[i] = sd(x_train[,i])
}

#Normalzing Feature Set 

x_train_n <- matrix(nrow = nrow(x_train), ncol = ncol(x_train))

for (n in 1:ncol(x_train)){
  for (m in 1:nrow(x_train)){
    x_train_n[m, n] = (x_train[m, n] - feature.means[n]) / feature.sds[n]
  }
}

#test data 
x_test_n <- matrix(nrow = nrow(x_test), ncol = ncol(x_test))
for (n in 1:ncol(x_test)){
  for (m in 1:nrow(x_test)){
    x_test_n[m, n] = (x_test[m, n] - feature.means[n]) / feature.sds[n]
  }
}


#setting up additional model stuff

init_w = initializer_random_normal(mean = 0,stddev = 0.05,seed = 123)

baseline_model <- 
  keras_model_sequential() %>%
   layer_dense(name = "DeepLayer1",
              units = 50 ,
              activation = "relu",
              input_shape = c(152),
              kernel_initializer =init_w)  %>% 
  layer_dense(name = "DeepLayer2",
              units = 50 ,
              activation = "relu") %>% 
  layer_dense(name = "OutputLayer",
              units = 20 ,
              activation = "relu")

baseline_model %>% compile(optimizer = optimizer_rmsprop(lr = 0.001, rho = 0.9), loss = "mean_absolute_error", metrics = list("mean_absolute_error") )

baseline_fit <- baseline_model %>% fit(x_train_n, y_train, epochs = 128, batchsize = 512, validation_data = list(x_test_n, y_test), verbose = 2 )

```


```{r}
#Training Run

tfruns::training_run(file = "FILE WITH PREVIOUS MODELS.R")

latest_run()

training_run(file = "FILE WITH PREVIOUS MODELS.R")

compare_runs()

training_run(file = "FILE WITH PREVIOUS MODELS.R")

compare_runs()
```

```{r}
#Tuning Hyperparameters 
  # add dropout and regularization
# Try adding and subtracting different layers 



```

```{r}
#Initialize model
model <- keras_model_sequential()

model %>%

  # Begin with 2D convolutional LSTM layer
  layer_conv_lstm_2d(
    input_shape = list(NULL,40,40,1), 
    filters = 40, kernel_size = c(3,3),
    padding = "same", 
    return_sequences = TRUE
  ) %>%
  # Normalize the activations of the previous layer
  layer_batch_normalization() %>%
  
  # Add 3x hidden 2D convolutions LSTM layers, with
  # batch normalization layers between
  layer_conv_lstm_2d(
    filters = 40, kernel_size = c(3,3),
    padding = "same", return_sequences = TRUE
  ) %>%
  layer_batch_normalization() %>%
  layer_conv_lstm_2d(
    filters = 40, kernel_size = c(3,3),
    padding = "same", return_sequences = TRUE
  ) %>%
  layer_batch_normalization() %>% 
  layer_conv_lstm_2d(
    filters = 40, kernel_size = c(3,3),
    padding = "same", return_sequences = TRUE
  ) %>%
  layer_batch_normalization() %>%
  
  # Add final 3D convolutional output layer 
  layer_conv_3d(
    filters = 1, kernel_size = c(3,3,3),
    activation = "sigmoid", 
    padding = "same", data_format ="channels_last"
  )

# Prepare model for training
model %>% compile(
  loss = "binary_crossentropy", 
  optimizer = "adadelta"
)

model
```

